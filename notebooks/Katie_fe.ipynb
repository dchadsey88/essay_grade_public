{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import stanza\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.abspath('..')\n",
    "sys.path.append(path)\n",
    "\n",
    "from essay_grader.lemmatize import lemmatize\n",
    "from essay_grader.punctuation import clean_string\n",
    "from essay_grader.bibliography import remove_all\n",
    "from essay_grader.vectorize import doc2vec_vectorize\n",
    "from essay_grader.text_feature import gen_text_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../essay_grader/data/essay_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_all(df)\n",
    "df = clean_string(df,column='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the question is asking that in the same discip...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 5 - Est_Chen-fzn235-TOK_essay.docx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our brains seek coherence structure and order ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Eva GuoTOK_final_final_draft.docx</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in american heritage dictionary of the english...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the statement in the prompt argues that diffic...</td>\n",
       "      <td>2017</td>\n",
       "      <td>8, 1 - James Li TOK_Essay_4th_draft.docx</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>It is only knowledge produced with difficulty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human are patternseeking animals because patte...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year  \\\n",
       "0  the question is asking that in the same discip...  2017   \n",
       "1  our brains seek coherence structure and order ...  2017   \n",
       "2  in american heritage dictionary of the english...  2017   \n",
       "3  the statement in the prompt argues that diffic...  2017   \n",
       "4  human are patternseeking animals because patte...  2017   \n",
       "\n",
       "                                                name  title  score  level  \\\n",
       "0              4, 5 - Est_Chen-fzn235-TOK_essay.docx      5      4      2   \n",
       "1           7, 6 - Eva GuoTOK_final_final_draft.docx      6      7      4   \n",
       "2  7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...      5      7      4   \n",
       "3           8, 1 - James Li TOK_Essay_4th_draft.docx      1      8      4   \n",
       "4  7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...      6      7      4   \n",
       "\n",
       "                                          title_name  \n",
       "0  Given access to the same facts, how is it poss...  \n",
       "1  Humans are pattern-seeking animals and we are ...  \n",
       "2  Given access to the same facts, how is it poss...  \n",
       "3  It is only knowledge produced with difficulty ...  \n",
       "4  Humans are pattern-seeking animals and we are ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    return unique_word_length/total_length\n",
    "\n",
    "df['vocab_richness'] = df.text.apply(vocab_richness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    0.311507\n",
       "2    0.309585\n",
       "3    0.309794\n",
       "4    0.314469\n",
       "5    0.320593\n",
       "Name: vocab_richness, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['vocab_richness'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    0.035250\n",
       "2    0.039863\n",
       "3    0.033980\n",
       "4    0.034732\n",
       "5    0.034370\n",
       "Name: vocab_richness, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['vocab_richness'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllables(word):\n",
    "    syllable_count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    if word[0] in vowels:\n",
    "        syllable_count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            syllable_count += 1\n",
    "    if word.endswith('e'):\n",
    "        syllable_count -= 1\n",
    "    if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:\n",
    "        syllable_count += 1\n",
    "    if syllable_count == 0:\n",
    "        syllable_count += 1\n",
    "    return syllable_count\n",
    "\n",
    "def mean_word_syllable(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    syl = 0\n",
    "    for token in tokens:\n",
    "        syl += syllables(token)\n",
    "    total_length = len(tokens)\n",
    "    return syl/total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "      <th>vocab_richness</th>\n",
       "      <th>meanWordSyllable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the question is asking that in the same discip...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 5 - Est_Chen-fzn235-TOK_essay.docx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.209360</td>\n",
       "      <td>1.540025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our brains seek coherence structure and order ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Eva GuoTOK_final_final_draft.docx</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>1.731380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in american heritage dictionary of the english...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>1.861711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the statement in the prompt argues that diffic...</td>\n",
       "      <td>2017</td>\n",
       "      <td>8, 1 - James Li TOK_Essay_4th_draft.docx</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>It is only knowledge produced with difficulty ...</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>1.779394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human are patternseeking animals because patte...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.254939</td>\n",
       "      <td>1.554493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year  \\\n",
       "0  the question is asking that in the same discip...  2017   \n",
       "1  our brains seek coherence structure and order ...  2017   \n",
       "2  in american heritage dictionary of the english...  2017   \n",
       "3  the statement in the prompt argues that diffic...  2017   \n",
       "4  human are patternseeking animals because patte...  2017   \n",
       "\n",
       "                                                name  title  score  level  \\\n",
       "0              4, 5 - Est_Chen-fzn235-TOK_essay.docx      5      4      2   \n",
       "1           7, 6 - Eva GuoTOK_final_final_draft.docx      6      7      4   \n",
       "2  7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...      5      7      4   \n",
       "3           8, 1 - James Li TOK_Essay_4th_draft.docx      1      8      4   \n",
       "4  7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...      6      7      4   \n",
       "\n",
       "                                          title_name  vocab_richness  \\\n",
       "0  Given access to the same facts, how is it poss...        0.209360   \n",
       "1  Humans are pattern-seeking animals and we are ...        0.344933   \n",
       "2  Given access to the same facts, how is it poss...        0.398082   \n",
       "3  It is only knowledge produced with difficulty ...        0.386667   \n",
       "4  Humans are pattern-seeking animals and we are ...        0.254939   \n",
       "\n",
       "   meanWordSyllable  \n",
       "0          1.540025  \n",
       "1          1.731380  \n",
       "2          1.861711  \n",
       "3          1.779394  \n",
       "4          1.554493  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['meanWordSyllable'] = df.text.apply(mean_word_syllable)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    1.663672\n",
       "2    1.691718\n",
       "3    1.713382\n",
       "4    1.737963\n",
       "5    1.715316\n",
       "Name: meanWordSyllable, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['meanWordSyllable'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    0.096444\n",
       "2    0.089836\n",
       "3    0.082089\n",
       "4    0.089888\n",
       "5    0.061876\n",
       "Name: meanWordSyllable, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['meanWordSyllable'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "# Splits the text into sentences, using  \n",
    "# Spacy's sentence segmentation which can  \n",
    "# be found at https://spacy.io/usage/spacy-101 \n",
    "nlp = spacy.load('en') \n",
    "\n",
    "def break_sentences(text, nlp): \n",
    "    doc = nlp(text) \n",
    "    return doc.sents \n",
    "  \n",
    "# Returns Number of Words in the text \n",
    "# def word_count(text): \n",
    "#     sentences = break_sentences(text,nlp) \n",
    "#     words = 0\n",
    "#     for sentence in sentences: \n",
    "#         words += len([token for token in sentence]) \n",
    "#     return words \n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split(\" \"))\n",
    "\n",
    "\n",
    "# Returns the number of sentences in the text \n",
    "# def sentence_count(text): \n",
    "#     sentences = break_sentences(text,nlp) \n",
    "#     return len(list(sentences))\n",
    "def sentence_count(text):\n",
    "    return len(text.split(\".\"))\n",
    "  \n",
    "# Returns average sentence length \n",
    "def avg_sentence_length(text): \n",
    "    words = word_count(text) \n",
    "    sentences = sentence_count(text) \n",
    "    average_sentence_length = float(words / sentences) \n",
    "    return average_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df.text.apply(word_count)\n",
    "df['sentence_count'] = df.text.apply(sentence_count)\n",
    "df['avg_sentence_length'] = df.text.apply(avg_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    1492.632653\n",
       "2    1485.168182\n",
       "3    1531.264151\n",
       "4    1536.000000\n",
       "5    1554.800000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['word_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    107.160257\n",
       "2    125.355582\n",
       "3     77.387113\n",
       "4     77.382784\n",
       "5     50.534147\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['word_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    71.673469\n",
       "2    68.550000\n",
       "3    67.905660\n",
       "4    67.515625\n",
       "5    61.200000\n",
       "Name: sentence_count, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['sentence_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    21.368343\n",
       "2    22.235258\n",
       "3    23.055620\n",
       "4    23.388957\n",
       "5    26.369376\n",
       "Name: avg_sentence_length, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")[\"avg_sentence_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    3.613049\n",
       "2    4.015905\n",
       "3    3.390360\n",
       "4    3.931143\n",
       "5    5.266054\n",
       "Name: avg_sentence_length, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")[\"avg_sentence_length\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def count_stopwords(x):\n",
    "    word_tokens = word_tokenize(x)\n",
    "    s = len([w for w in word_tokens if w in stop_words])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_stopwords'] = df.text.apply(count_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    690.510204\n",
       "2    678.327273\n",
       "3    691.220126\n",
       "4    691.421875\n",
       "5    698.200000\n",
       "Name: count_stopwords, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['count_stopwords'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    65.167452\n",
       "2    66.164885\n",
       "3    48.100850\n",
       "4    58.723405\n",
       "5    57.590798\n",
       "Name: count_stopwords, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")['count_stopwords'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A higher score in Flesch’s reading ease test indicates material that is easier to read; lower numbers mark passages that are more difficult to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat.textstat import textstatistics, legacy_round \n",
    "\n",
    "\n",
    "def flesch_reading_ease(text): \n",
    "    \"\"\" \n",
    "        Implements Flesch Formula: \n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW) \n",
    "        Here, \n",
    "          ASL = average sentence length (number of words  \n",
    "                divided by number of sentences) \n",
    "          ASW = average word length in syllables (number of syllables  \n",
    "                divided by number of words) \n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) -\\\n",
    "          float(84.6 * mean_word_syllable(text)) \n",
    "    return legacy_round(FRE, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reading_ease'] = df.text.apply(flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    44.399184\n",
       "2    41.146864\n",
       "3    38.481509\n",
       "4    36.063594\n",
       "5    34.952000\n",
       "Name: reading_ease, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")[\"reading_ease\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    9.141040\n",
       "2    9.460359\n",
       "3    8.251395\n",
       "4    9.652404\n",
       "5    9.682183\n",
       "Name: reading_ease, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")[\"reading_ease\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "      <th>vocab_richness</th>\n",
       "      <th>meanWordSyllable</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the question is asking that in the same discip...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 5 - Est_Chen-fzn235-TOK_essay.docx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.209360</td>\n",
       "      <td>1.540025</td>\n",
       "      <td>1525</td>\n",
       "      <td>94</td>\n",
       "      <td>16.223404</td>\n",
       "      <td>788</td>\n",
       "      <td>60.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our brains seek coherence structure and order ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Eva GuoTOK_final_final_draft.docx</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>1.731380</td>\n",
       "      <td>1575</td>\n",
       "      <td>58</td>\n",
       "      <td>27.155172</td>\n",
       "      <td>724</td>\n",
       "      <td>32.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in american heritage dictionary of the english...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>1.861711</td>\n",
       "      <td>1208</td>\n",
       "      <td>41</td>\n",
       "      <td>29.463415</td>\n",
       "      <td>528</td>\n",
       "      <td>19.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the statement in the prompt argues that diffic...</td>\n",
       "      <td>2017</td>\n",
       "      <td>8, 1 - James Li TOK_Essay_4th_draft.docx</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>It is only knowledge produced with difficulty ...</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>1.779394</td>\n",
       "      <td>1594</td>\n",
       "      <td>60</td>\n",
       "      <td>26.566667</td>\n",
       "      <td>732</td>\n",
       "      <td>29.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human are patternseeking animals because patte...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.254939</td>\n",
       "      <td>1.554493</td>\n",
       "      <td>1500</td>\n",
       "      <td>67</td>\n",
       "      <td>22.388060</td>\n",
       "      <td>703</td>\n",
       "      <td>52.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year  \\\n",
       "0  the question is asking that in the same discip...  2017   \n",
       "1  our brains seek coherence structure and order ...  2017   \n",
       "2  in american heritage dictionary of the english...  2017   \n",
       "3  the statement in the prompt argues that diffic...  2017   \n",
       "4  human are patternseeking animals because patte...  2017   \n",
       "\n",
       "                                                name  title  score  level  \\\n",
       "0              4, 5 - Est_Chen-fzn235-TOK_essay.docx      5      4      2   \n",
       "1           7, 6 - Eva GuoTOK_final_final_draft.docx      6      7      4   \n",
       "2  7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...      5      7      4   \n",
       "3           8, 1 - James Li TOK_Essay_4th_draft.docx      1      8      4   \n",
       "4  7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...      6      7      4   \n",
       "\n",
       "                                          title_name  vocab_richness  \\\n",
       "0  Given access to the same facts, how is it poss...        0.209360   \n",
       "1  Humans are pattern-seeking animals and we are ...        0.344933   \n",
       "2  Given access to the same facts, how is it poss...        0.398082   \n",
       "3  It is only knowledge produced with difficulty ...        0.386667   \n",
       "4  Humans are pattern-seeking animals and we are ...        0.254939   \n",
       "\n",
       "   meanWordSyllable  word_count  sentence_count  avg_sentence_length  \\\n",
       "0          1.540025        1525              94            16.223404   \n",
       "1          1.731380        1575              58            27.155172   \n",
       "2          1.861711        1208              41            29.463415   \n",
       "3          1.779394        1594              60            26.566667   \n",
       "4          1.554493        1500              67            22.388060   \n",
       "\n",
       "   count_stopwords  reading_ease  \n",
       "0              788         60.08  \n",
       "1              724         32.80  \n",
       "2              528         19.43  \n",
       "3              732         29.33  \n",
       "4              703         52.60  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle('../essay_grader/pickle_data/df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.abspath('..')\n",
    "sys.path.append(path)\n",
    "\n",
    "from essay_grader.lemmatize import lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../essay_grader/pickle_data/df.pkl\", \"rb\") as file:\n",
    "        df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-21 09:36:57 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-08-21 09:36:57 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-08-21 09:36:57 INFO: Use device: cpu\n",
      "2020-08-21 09:36:57 INFO: Loading: tokenize\n",
      "2020-08-21 09:36:57 INFO: Loading: pos\n",
      "2020-08-21 09:36:58 INFO: Loading: lemma\n",
      "2020-08-21 09:36:58 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "df = lemmatize(df,\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "      <th>vocab_richness</th>\n",
       "      <th>meanWordSyllable</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>reading_ease</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the question is asking that in the same discip...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 5 - Est_Chen-fzn235-TOK_essay.docx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.209360</td>\n",
       "      <td>1.540025</td>\n",
       "      <td>1525</td>\n",
       "      <td>94</td>\n",
       "      <td>16.223404</td>\n",
       "      <td>788</td>\n",
       "      <td>60.08</td>\n",
       "      <td>[[the, question, be, ask, that, in, the, same,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our brains seek coherence structure and order ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Eva GuoTOK_final_final_draft.docx</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>1.731380</td>\n",
       "      <td>1575</td>\n",
       "      <td>58</td>\n",
       "      <td>27.155172</td>\n",
       "      <td>724</td>\n",
       "      <td>32.80</td>\n",
       "      <td>[[we, brain, seek, coherence, structure, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in american heritage dictionary of the english...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>1.861711</td>\n",
       "      <td>1208</td>\n",
       "      <td>41</td>\n",
       "      <td>29.463415</td>\n",
       "      <td>528</td>\n",
       "      <td>19.43</td>\n",
       "      <td>[[in, american, heritage, dictionary, of, the,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year  \\\n",
       "0  the question is asking that in the same discip...  2017   \n",
       "1  our brains seek coherence structure and order ...  2017   \n",
       "2  in american heritage dictionary of the english...  2017   \n",
       "\n",
       "                                                name  title  score  level  \\\n",
       "0              4, 5 - Est_Chen-fzn235-TOK_essay.docx      5      4      2   \n",
       "1           7, 6 - Eva GuoTOK_final_final_draft.docx      6      7      4   \n",
       "2  7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...      5      7      4   \n",
       "\n",
       "                                          title_name  vocab_richness  \\\n",
       "0  Given access to the same facts, how is it poss...        0.209360   \n",
       "1  Humans are pattern-seeking animals and we are ...        0.344933   \n",
       "2  Given access to the same facts, how is it poss...        0.398082   \n",
       "\n",
       "   meanWordSyllable  word_count  sentence_count  avg_sentence_length  \\\n",
       "0          1.540025        1525              94            16.223404   \n",
       "1          1.731380        1575              58            27.155172   \n",
       "2          1.861711        1208              41            29.463415   \n",
       "\n",
       "   count_stopwords  reading_ease  \\\n",
       "0              788         60.08   \n",
       "1              724         32.80   \n",
       "2              528         19.43   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [[the, question, be, ask, that, in, the, same,...  \n",
       "1  [[we, brain, seek, coherence, structure, and, ...  \n",
       "2  [[in, american, heritage, dictionary, of, the,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle('../essay_grader/pickle_data/lemma_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.abspath('..')\n",
    "sys.path.append(path)\n",
    "\n",
    "from essay_grader.text_feature import gen_text_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The claim suggests that our degree of apprecia...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 1 - FZN289_Nick_Zhang_Zexun_G12-8_TOKEssay_...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>It is only knowledge produced with difficulty ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  year  \\\n",
       "30  The claim suggests that our degree of apprecia...  2017   \n",
       "\n",
       "                                                 name  title  score  level  \\\n",
       "30  4, 1 - FZN289_Nick_Zhang_Zexun_G12-8_TOKEssay_...      1      4      2   \n",
       "\n",
       "                                           title_name  \n",
       "30  It is only knowledge produced with difficulty ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "      <th>vocab_richness</th>\n",
       "      <th>mean_word_syllable</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the question is asking that in the same discip...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 5 - Est_Chen-fzn235-TOK_essay.docx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.209360</td>\n",
       "      <td>1.540025</td>\n",
       "      <td>1525</td>\n",
       "      <td>94</td>\n",
       "      <td>16.223404</td>\n",
       "      <td>788</td>\n",
       "      <td>60.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our brains seek coherence structure and order ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Eva GuoTOK_final_final_draft.docx</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>1.731380</td>\n",
       "      <td>1575</td>\n",
       "      <td>58</td>\n",
       "      <td>27.155172</td>\n",
       "      <td>724</td>\n",
       "      <td>32.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in american heritage dictionary of the english...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>1.861711</td>\n",
       "      <td>1208</td>\n",
       "      <td>41</td>\n",
       "      <td>29.463415</td>\n",
       "      <td>528</td>\n",
       "      <td>19.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the statement in the prompt argues that diffic...</td>\n",
       "      <td>2017</td>\n",
       "      <td>8, 1 - James Li TOK_Essay_4th_draft.docx</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>It is only knowledge produced with difficulty ...</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>1.779394</td>\n",
       "      <td>1594</td>\n",
       "      <td>60</td>\n",
       "      <td>26.566667</td>\n",
       "      <td>732</td>\n",
       "      <td>29.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human are patternseeking animals because patte...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.254939</td>\n",
       "      <td>1.554493</td>\n",
       "      <td>1500</td>\n",
       "      <td>67</td>\n",
       "      <td>22.388060</td>\n",
       "      <td>703</td>\n",
       "      <td>52.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year  \\\n",
       "0  the question is asking that in the same discip...  2017   \n",
       "1  our brains seek coherence structure and order ...  2017   \n",
       "2  in american heritage dictionary of the english...  2017   \n",
       "3  the statement in the prompt argues that diffic...  2017   \n",
       "4  human are patternseeking animals because patte...  2017   \n",
       "\n",
       "                                                name  title  score  level  \\\n",
       "0              4, 5 - Est_Chen-fzn235-TOK_essay.docx      5      4      2   \n",
       "1           7, 6 - Eva GuoTOK_final_final_draft.docx      6      7      4   \n",
       "2  7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...      5      7      4   \n",
       "3           8, 1 - James Li TOK_Essay_4th_draft.docx      1      8      4   \n",
       "4  7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...      6      7      4   \n",
       "\n",
       "                                          title_name  vocab_richness  \\\n",
       "0  Given access to the same facts, how is it poss...        0.209360   \n",
       "1  Humans are pattern-seeking animals and we are ...        0.344933   \n",
       "2  Given access to the same facts, how is it poss...        0.398082   \n",
       "3  It is only knowledge produced with difficulty ...        0.386667   \n",
       "4  Humans are pattern-seeking animals and we are ...        0.254939   \n",
       "\n",
       "   mean_word_syllable  word_count  sentence_count  avg_sentence_length  \\\n",
       "0            1.540025        1525              94            16.223404   \n",
       "1            1.731380        1575              58            27.155172   \n",
       "2            1.861711        1208              41            29.463415   \n",
       "3            1.779394        1594              60            26.566667   \n",
       "4            1.554493        1500              67            22.388060   \n",
       "\n",
       "   count_stopwords  flesch_reading_ease  \n",
       "0              788                60.08  \n",
       "1              724                32.80  \n",
       "2              528                19.43  \n",
       "3              732                29.33  \n",
       "4              703                52.60  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = gen_text_feature(df)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with gensim word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load pretrained model \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path +\\\n",
    "    '/essay_grader/data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998437"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_obj = model.vocab[\"word\"]\n",
    "vocab_obj.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998091"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_obj = model.vocab[\"knowledge\"]\n",
    "vocab_obj.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999506"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_obj = model.vocab[\"Obama\"]\n",
    "vocab_obj.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../essay_grader/pickle_data/lemma_df.pkl\", \"rb\") as file:\n",
    "        df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "      <th>vocab_richness</th>\n",
       "      <th>meanWordSyllable</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>reading_ease</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the question is asking that in the same discip...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 5 - Est_Chen-fzn235-TOK_essay.docx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.209360</td>\n",
       "      <td>1.540025</td>\n",
       "      <td>1525</td>\n",
       "      <td>94</td>\n",
       "      <td>16.223404</td>\n",
       "      <td>788</td>\n",
       "      <td>60.08</td>\n",
       "      <td>[[the, question, be, ask, that, in, the, same,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our brains seek coherence structure and order ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Eva GuoTOK_final_final_draft.docx</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>1.731380</td>\n",
       "      <td>1575</td>\n",
       "      <td>58</td>\n",
       "      <td>27.155172</td>\n",
       "      <td>724</td>\n",
       "      <td>32.80</td>\n",
       "      <td>[[we, brain, seek, coherence, structure, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in american heritage dictionary of the english...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>1.861711</td>\n",
       "      <td>1208</td>\n",
       "      <td>41</td>\n",
       "      <td>29.463415</td>\n",
       "      <td>528</td>\n",
       "      <td>19.43</td>\n",
       "      <td>[[in, american, heritage, dictionary, of, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the statement in the prompt argues that diffic...</td>\n",
       "      <td>2017</td>\n",
       "      <td>8, 1 - James Li TOK_Essay_4th_draft.docx</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>It is only knowledge produced with difficulty ...</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>1.779394</td>\n",
       "      <td>1594</td>\n",
       "      <td>60</td>\n",
       "      <td>26.566667</td>\n",
       "      <td>732</td>\n",
       "      <td>29.33</td>\n",
       "      <td>[[the, statement, in, the, prompt, argue, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human are patternseeking animals because patte...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.254939</td>\n",
       "      <td>1.554493</td>\n",
       "      <td>1500</td>\n",
       "      <td>67</td>\n",
       "      <td>22.388060</td>\n",
       "      <td>703</td>\n",
       "      <td>52.60</td>\n",
       "      <td>[[human, be, patternseek, animal, because, pat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year  \\\n",
       "0  the question is asking that in the same discip...  2017   \n",
       "1  our brains seek coherence structure and order ...  2017   \n",
       "2  in american heritage dictionary of the english...  2017   \n",
       "3  the statement in the prompt argues that diffic...  2017   \n",
       "4  human are patternseeking animals because patte...  2017   \n",
       "\n",
       "                                                name  title  score  level  \\\n",
       "0              4, 5 - Est_Chen-fzn235-TOK_essay.docx      5      4      2   \n",
       "1           7, 6 - Eva GuoTOK_final_final_draft.docx      6      7      4   \n",
       "2  7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...      5      7      4   \n",
       "3           8, 1 - James Li TOK_Essay_4th_draft.docx      1      8      4   \n",
       "4  7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...      6      7      4   \n",
       "\n",
       "                                          title_name  vocab_richness  \\\n",
       "0  Given access to the same facts, how is it poss...        0.209360   \n",
       "1  Humans are pattern-seeking animals and we are ...        0.344933   \n",
       "2  Given access to the same facts, how is it poss...        0.398082   \n",
       "3  It is only knowledge produced with difficulty ...        0.386667   \n",
       "4  Humans are pattern-seeking animals and we are ...        0.254939   \n",
       "\n",
       "   meanWordSyllable  word_count  sentence_count  avg_sentence_length  \\\n",
       "0          1.540025        1525              94            16.223404   \n",
       "1          1.731380        1575              58            27.155172   \n",
       "2          1.861711        1208              41            29.463415   \n",
       "3          1.779394        1594              60            26.566667   \n",
       "4          1.554493        1500              67            22.388060   \n",
       "\n",
       "   count_stopwords  reading_ease  \\\n",
       "0              788         60.08   \n",
       "1              724         32.80   \n",
       "2              528         19.43   \n",
       "3              732         29.33   \n",
       "4              703         52.60   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [[the, question, be, ask, that, in, the, same,...  \n",
       "1  [[we, brain, seek, coherence, structure, and, ...  \n",
       "2  [[in, american, heritage, dictionary, of, the,...  \n",
       "3  [[the, statement, in, the, prompt, argue, that...  \n",
       "4  [[human, be, patternseek, animal, because, pat...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_commonness(lemmatized_list):\n",
    "    sum_count = 0\n",
    "    length_count = 0\n",
    "    for i in lemmatized_list:\n",
    "        for j in i:\n",
    "            try:\n",
    "                vocab_obj = model.vocab[j]\n",
    "                sum_count += vocab_obj.count\n",
    "                length_count += 1\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "    return sum_count/length_count      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_commonness\"] = df.lemmatized_text.apply(word_commonness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "      <th>vocab_richness</th>\n",
       "      <th>meanWordSyllable</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>reading_ease</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>word_commonness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the question is asking that in the same discip...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 5 - Est_Chen-fzn235-TOK_essay.docx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.209360</td>\n",
       "      <td>1.540025</td>\n",
       "      <td>1525</td>\n",
       "      <td>94</td>\n",
       "      <td>16.223404</td>\n",
       "      <td>788</td>\n",
       "      <td>60.08</td>\n",
       "      <td>[[the, question, be, ask, that, in, the, same,...</td>\n",
       "      <td>2.992275e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our brains seek coherence structure and order ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Eva GuoTOK_final_final_draft.docx</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>1.731380</td>\n",
       "      <td>1575</td>\n",
       "      <td>58</td>\n",
       "      <td>27.155172</td>\n",
       "      <td>724</td>\n",
       "      <td>32.80</td>\n",
       "      <td>[[we, brain, seek, coherence, structure, and, ...</td>\n",
       "      <td>2.988191e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in american heritage dictionary of the english...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>1.861711</td>\n",
       "      <td>1208</td>\n",
       "      <td>41</td>\n",
       "      <td>29.463415</td>\n",
       "      <td>528</td>\n",
       "      <td>19.43</td>\n",
       "      <td>[[in, american, heritage, dictionary, of, the,...</td>\n",
       "      <td>2.989039e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year  \\\n",
       "0  the question is asking that in the same discip...  2017   \n",
       "1  our brains seek coherence structure and order ...  2017   \n",
       "2  in american heritage dictionary of the english...  2017   \n",
       "\n",
       "                                                name  title  score  level  \\\n",
       "0              4, 5 - Est_Chen-fzn235-TOK_essay.docx      5      4      2   \n",
       "1           7, 6 - Eva GuoTOK_final_final_draft.docx      6      7      4   \n",
       "2  7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...      5      7      4   \n",
       "\n",
       "                                          title_name  vocab_richness  \\\n",
       "0  Given access to the same facts, how is it poss...        0.209360   \n",
       "1  Humans are pattern-seeking animals and we are ...        0.344933   \n",
       "2  Given access to the same facts, how is it poss...        0.398082   \n",
       "\n",
       "   meanWordSyllable  word_count  sentence_count  avg_sentence_length  \\\n",
       "0          1.540025        1525              94            16.223404   \n",
       "1          1.731380        1575              58            27.155172   \n",
       "2          1.861711        1208              41            29.463415   \n",
       "\n",
       "   count_stopwords  reading_ease  \\\n",
       "0              788         60.08   \n",
       "1              724         32.80   \n",
       "2              528         19.43   \n",
       "\n",
       "                                     lemmatized_text  word_commonness  \n",
       "0  [[the, question, be, ask, that, in, the, same,...     2.992275e+06  \n",
       "1  [[we, brain, seek, coherence, structure, and, ...     2.988191e+06  \n",
       "2  [[in, american, heritage, dictionary, of, the,...     2.989039e+06  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    9.960910\n",
       "2    9.958811\n",
       "3    9.956124\n",
       "4    9.955638\n",
       "5    9.945923\n",
       "Name: word_commonness, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")[\"word_commonness\"].mean()/300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.988273e+06</td>\n",
       "      <td>6681.194699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.987643e+06</td>\n",
       "      <td>6337.604019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.986837e+06</td>\n",
       "      <td>6189.552744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.986691e+06</td>\n",
       "      <td>6458.871722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.983777e+06</td>\n",
       "      <td>7951.331454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean          std\n",
       "level                           \n",
       "1      2.988273e+06  6681.194699\n",
       "2      2.987643e+06  6337.604019\n",
       "3      2.986837e+06  6189.552744\n",
       "4      2.986691e+06  6458.871722\n",
       "5      2.983777e+06  7951.331454"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")[\"word_commonness\"].agg([\"mean\",\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text0 = df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the question is asking that in the same discipline why experts often have different opinions on the same fact for example a scientist disagrees with other scientists discovers and results facts are th'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text0[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text0 = text0.replace(\".\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text0.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_commonness(text):\n",
    "    sum_count = 0\n",
    "    length_count = 0\n",
    "    text = text.replace(\".\",\"\")\n",
    "    text_list = text.split(\" \")\n",
    "    for i in text_list:\n",
    "        try:\n",
    "            vocab_obj = model.vocab[i]\n",
    "            sum_count += vocab_obj.count\n",
    "            length_count += 1\n",
    "        except KeyError as e:\n",
    "            pass\n",
    "    return sum_count/length_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_commonness_2\"] = df.text.apply(word_commonness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    0.995750\n",
       "2    0.995544\n",
       "3    0.995182\n",
       "4    0.995156\n",
       "5    0.991894\n",
       "Name: word_commonness_2, dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")[\"word_commonness_2\"].mean()/3000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "1    0.002130\n",
       "2    0.002242\n",
       "3    0.002630\n",
       "4    0.002440\n",
       "5    0.005383\n",
       "Name: word_commonness_2, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"level\")[\"word_commonness_2\"].std()/3000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test text_feature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../essay_grader/pickle_data/lemma_df.pkl\", \"rb\") as file:\n",
    "        lemma_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'question',\n",
       "  'be',\n",
       "  'ask',\n",
       "  'that',\n",
       "  'in',\n",
       "  'the',\n",
       "  'same',\n",
       "  'discipline',\n",
       "  'why',\n",
       "  'expert',\n",
       "  'often',\n",
       "  'have',\n",
       "  'different',\n",
       "  'opinion',\n",
       "  'on',\n",
       "  'the',\n",
       "  'same',\n",
       "  'fact',\n",
       "  '.'],\n",
       " ['for',\n",
       "  'example',\n",
       "  'a',\n",
       "  'scientist',\n",
       "  'disagree',\n",
       "  'with',\n",
       "  'other',\n",
       "  'scientist',\n",
       "  'discover',\n",
       "  'and',\n",
       "  'result',\n",
       "  '.'],\n",
       " ['fact',\n",
       "  'be',\n",
       "  'the',\n",
       "  'phenomena',\n",
       "  'that',\n",
       "  'can',\n",
       "  'be',\n",
       "  'commonly',\n",
       "  'observe',\n",
       "  'by',\n",
       "  'we',\n",
       "  'and',\n",
       "  'can',\n",
       "  'not',\n",
       "  'be',\n",
       "  'change',\n",
       "  'by',\n",
       "  'we',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'be',\n",
       "  'the',\n",
       "  'evidence',\n",
       "  'and',\n",
       "  'the',\n",
       "  'basic',\n",
       "  'knowledge',\n",
       "  'that',\n",
       "  'we',\n",
       "  'can',\n",
       "  'easily',\n",
       "  'get',\n",
       "  '.'],\n",
       " ['for', 'example', 'the', 'earth', 'be', 'a', 'sphere', '.'],\n",
       " ['this',\n",
       "  'be',\n",
       "  'a',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'we',\n",
       "  'already',\n",
       "  'prove',\n",
       "  'and',\n",
       "  'observe',\n",
       "  'from',\n",
       "  'space',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'evidence',\n",
       "  'be',\n",
       "  'sufficient',\n",
       "  'to',\n",
       "  'prove',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'by',\n",
       "  'use',\n",
       "  'reasoning',\n",
       "  'as',\n",
       "  'a',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  'and',\n",
       "  'we',\n",
       "  'now',\n",
       "  'treat',\n",
       "  'this',\n",
       "  'fact',\n",
       "  'as',\n",
       "  'knowledge',\n",
       "  '.'],\n",
       " ['that', 'be', 'how', 'we', 'gain', 'knowledge', 'from', 'fact', '.'],\n",
       " ['however',\n",
       "  'fact',\n",
       "  'be',\n",
       "  'not',\n",
       "  'the',\n",
       "  'same',\n",
       "  'in',\n",
       "  'all',\n",
       "  'discipline',\n",
       "  '.'],\n",
       " ['base',\n",
       "  'on',\n",
       "  'the',\n",
       "  'different',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  'use',\n",
       "  'in',\n",
       "  'different',\n",
       "  'area',\n",
       "  'of',\n",
       "  'knowledge',\n",
       "  'fact',\n",
       "  'be',\n",
       "  'evaluate',\n",
       "  'differently',\n",
       "  '.'],\n",
       " ['argument',\n",
       "  'be',\n",
       "  'mainly',\n",
       "  'cause',\n",
       "  'by',\n",
       "  'the',\n",
       "  'different',\n",
       "  'opinion',\n",
       "  'or',\n",
       "  'knowledge',\n",
       "  'between',\n",
       "  'two',\n",
       "  'party',\n",
       "  '.'],\n",
       " ['two',\n",
       "  'expert',\n",
       "  'may',\n",
       "  'have',\n",
       "  'disagreement',\n",
       "  'on',\n",
       "  'the',\n",
       "  'understanding',\n",
       "  'of',\n",
       "  'the',\n",
       "  'same',\n",
       "  'phenomenon',\n",
       "  '.'],\n",
       " ['expert',\n",
       "  'in',\n",
       "  'the',\n",
       "  'same',\n",
       "  'discipline',\n",
       "  'consist',\n",
       "  'of',\n",
       "  'different',\n",
       "  'individual',\n",
       "  'so',\n",
       "  'they',\n",
       "  'think',\n",
       "  'differently',\n",
       "  'which',\n",
       "  'cause',\n",
       "  'by',\n",
       "  'many',\n",
       "  'factor',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'will',\n",
       "  'have',\n",
       "  'many',\n",
       "  'different',\n",
       "  'opinion',\n",
       "  'on',\n",
       "  'the',\n",
       "  'same',\n",
       "  'phenomena',\n",
       "  'because',\n",
       "  'the',\n",
       "  'difference',\n",
       "  'of',\n",
       "  'use',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  'when',\n",
       "  'expert',\n",
       "  'gain',\n",
       "  'knowledge',\n",
       "  'from',\n",
       "  'fact',\n",
       "  '.'],\n",
       " ['when',\n",
       "  'expert',\n",
       "  'have',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  'to',\n",
       "  'each',\n",
       "  'other',\n",
       "  'they',\n",
       "  'will',\n",
       "  'compare',\n",
       "  'they',\n",
       "  'knowledge',\n",
       "  'and',\n",
       "  'have',\n",
       "  'argument',\n",
       "  'to',\n",
       "  'find',\n",
       "  'the',\n",
       "  'truth',\n",
       "  'knowledge',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'be',\n",
       "  'the',\n",
       "  'general',\n",
       "  'process',\n",
       "  'of',\n",
       "  'how',\n",
       "  'a',\n",
       "  'argument',\n",
       "  'form',\n",
       "  '.'],\n",
       " ['also',\n",
       "  'in',\n",
       "  'different',\n",
       "  'area',\n",
       "  'of',\n",
       "  'knowledge',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'and',\n",
       "  'mathematics',\n",
       "  'the',\n",
       "  'argument',\n",
       "  'may',\n",
       "  'be',\n",
       "  'cause',\n",
       "  'differently',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'main',\n",
       "  'question',\n",
       "  'can',\n",
       "  'be',\n",
       "  'separate',\n",
       "  'into',\n",
       "  'two',\n",
       "  'knowledge',\n",
       "  'question',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'first',\n",
       "  'one',\n",
       "  'be',\n",
       "  'how',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  'can',\n",
       "  'be',\n",
       "  'different',\n",
       "  'from',\n",
       "  'the',\n",
       "  'same',\n",
       "  'fact',\n",
       "  'between',\n",
       "  'expert',\n",
       "  'in',\n",
       "  'the',\n",
       "  'same',\n",
       "  'discipline',\n",
       "  '.'],\n",
       " ['expert', 'in', 'this', 'area', 'of', 'knowledge', 'be', 'scientist', '.'],\n",
       " ['science',\n",
       "  'be',\n",
       "  'consider',\n",
       "  'as',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'reliable',\n",
       "  'knowledge',\n",
       "  'because',\n",
       "  'it',\n",
       "  'base',\n",
       "  'on',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'evidence',\n",
       "  '.'],\n",
       " ['fact',\n",
       "  'the',\n",
       "  'evidence',\n",
       "  'be',\n",
       "  'the',\n",
       "  'center',\n",
       "  'of',\n",
       "  'the',\n",
       "  'argument',\n",
       "  'between',\n",
       "  'the',\n",
       "  'expert',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'mean',\n",
       "  'basically',\n",
       "  'they',\n",
       "  'will',\n",
       "  'use',\n",
       "  'reasoning',\n",
       "  'as',\n",
       "  'a',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  '.'],\n",
       " ['however',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'the',\n",
       "  'argument',\n",
       "  'and',\n",
       "  'disagreement',\n",
       "  'always',\n",
       "  'exist',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'my',\n",
       "  'ib',\n",
       "  'biology',\n",
       "  'class',\n",
       "  'although',\n",
       "  'we',\n",
       "  'study',\n",
       "  'the',\n",
       "  'theory',\n",
       "  'of',\n",
       "  'evolution',\n",
       "  'but',\n",
       "  'most',\n",
       "  'of',\n",
       "  'my',\n",
       "  'classmate',\n",
       "  'and',\n",
       "  'even',\n",
       "  'my',\n",
       "  'teacher',\n",
       "  'do',\n",
       "  'not',\n",
       "  'agree',\n",
       "  'with',\n",
       "  'the',\n",
       "  'certainty',\n",
       "  'of',\n",
       "  'the',\n",
       "  'evolution',\n",
       "  'theory',\n",
       "  'which',\n",
       "  'mean',\n",
       "  'what',\n",
       "  'be',\n",
       "  'accept',\n",
       "  'by',\n",
       "  'some',\n",
       "  'group',\n",
       "  'of',\n",
       "  'scientist',\n",
       "  'may',\n",
       "  'not',\n",
       "  'be',\n",
       "  'accept',\n",
       "  'by',\n",
       "  'other',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'these',\n",
       "  'case',\n",
       "  'it',\n",
       "  'seem',\n",
       "  'that',\n",
       "  'the',\n",
       "  'claim',\n",
       "  'be',\n",
       "  'correct',\n",
       "  '.'],\n",
       " ['base',\n",
       "  'on',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'to',\n",
       "  'prove',\n",
       "  'a',\n",
       "  'theory',\n",
       "  'to',\n",
       "  'be',\n",
       "  'wrong',\n",
       "  'need',\n",
       "  'evidence',\n",
       "  'while',\n",
       "  'to',\n",
       "  'support',\n",
       "  'still',\n",
       "  'need',\n",
       "  'evidence',\n",
       "  '.'],\n",
       " ['that',\n",
       "  'be',\n",
       "  'because',\n",
       "  'in',\n",
       "  'the',\n",
       "  'area',\n",
       "  'of',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'we',\n",
       "  'use',\n",
       "  'reasoning',\n",
       "  'as',\n",
       "  'a',\n",
       "  'important',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  '.'],\n",
       " ['about',\n",
       "  'evolution',\n",
       "  'theory',\n",
       "  'we',\n",
       "  'only',\n",
       "  'have',\n",
       "  'few',\n",
       "  'direct',\n",
       "  'evidence',\n",
       "  'to',\n",
       "  'prove',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'certainty',\n",
       "  'but',\n",
       "  'this',\n",
       "  'evidence',\n",
       "  'we',\n",
       "  'have',\n",
       "  'show',\n",
       "  'the',\n",
       "  'evolution',\n",
       "  'theory',\n",
       "  'be',\n",
       "  'the',\n",
       "  'possible',\n",
       "  'knowledge',\n",
       "  'to',\n",
       "  'be',\n",
       "  'correct',\n",
       "  'which',\n",
       "  'mean',\n",
       "  'it',\n",
       "  'be',\n",
       "  'the',\n",
       "  'most',\n",
       "  'logical',\n",
       "  'result',\n",
       "  'we',\n",
       "  'have',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'knowledge',\n",
       "  'be',\n",
       "  'also',\n",
       "  'now',\n",
       "  'link',\n",
       "  'with',\n",
       "  'imagination',\n",
       "  'and',\n",
       "  'intuition',\n",
       "  'as',\n",
       "  'a',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'when',\n",
       "  'expert',\n",
       "  'can',\n",
       "  'not',\n",
       "  'percent',\n",
       "  'relay',\n",
       "  'on',\n",
       "  'reasoning',\n",
       "  'to',\n",
       "  'gain',\n",
       "  'knowledge',\n",
       "  'they',\n",
       "  'will',\n",
       "  'use',\n",
       "  'intuition',\n",
       "  'or',\n",
       "  'imagination',\n",
       "  'to',\n",
       "  'gain',\n",
       "  'knowledge',\n",
       "  'from',\n",
       "  'a',\n",
       "  'fact',\n",
       "  '.'],\n",
       " ['and',\n",
       "  'different',\n",
       "  'scientist',\n",
       "  'may',\n",
       "  'use',\n",
       "  'different',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  'to',\n",
       "  'gain',\n",
       "  'knowledge',\n",
       "  '.'],\n",
       " ['some',\n",
       "  'scientist',\n",
       "  'may',\n",
       "  'relay',\n",
       "  'on',\n",
       "  'reasoning',\n",
       "  'more',\n",
       "  'so',\n",
       "  'they',\n",
       "  'do',\n",
       "  'not',\n",
       "  'agree',\n",
       "  'with',\n",
       "  'evolution',\n",
       "  'theory',\n",
       "  '.'],\n",
       " ['some',\n",
       "  'may',\n",
       "  'relay',\n",
       "  'on',\n",
       "  'intuition',\n",
       "  'and',\n",
       "  'imagination',\n",
       "  'more',\n",
       "  'than',\n",
       "  'reasoning',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'cause',\n",
       "  'the',\n",
       "  'big',\n",
       "  'difference',\n",
       "  'between',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  'they',\n",
       "  'gain',\n",
       "  'from',\n",
       "  'the',\n",
       "  'same',\n",
       "  'fact',\n",
       "  '.'],\n",
       " ['to',\n",
       "  'prove',\n",
       "  'a',\n",
       "  'theory',\n",
       "  'be',\n",
       "  'wrong',\n",
       "  'a',\n",
       "  'scientist',\n",
       "  'can',\n",
       "  'just',\n",
       "  'show',\n",
       "  'a',\n",
       "  'counterclaim',\n",
       "  'with',\n",
       "  'enough',\n",
       "  'evidence',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'will',\n",
       "  'ruin',\n",
       "  'the',\n",
       "  'logic',\n",
       "  'and',\n",
       "  'be',\n",
       "  'give',\n",
       "  'reason',\n",
       "  'to',\n",
       "  'complete',\n",
       "  'a',\n",
       "  'falsification',\n",
       "  '.'],\n",
       " ['thus',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  'be',\n",
       "  'form',\n",
       "  'between',\n",
       "  'different',\n",
       "  'expert',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'science',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'my',\n",
       "  'ib',\n",
       "  'chemistry',\n",
       "  'class',\n",
       "  'the',\n",
       "  'graph',\n",
       "  'and',\n",
       "  'model',\n",
       "  'be',\n",
       "  'always',\n",
       "  'use',\n",
       "  'to',\n",
       "  'analyze',\n",
       "  'different',\n",
       "  'situation',\n",
       "  '.'],\n",
       " ['we',\n",
       "  'create',\n",
       "  'model',\n",
       "  'to',\n",
       "  'explain',\n",
       "  'some',\n",
       "  'phenomenon',\n",
       "  'we',\n",
       "  'can',\n",
       "  'not',\n",
       "  'direct',\n",
       "  'observe',\n",
       "  '.'],\n",
       " ['there',\n",
       "  'may',\n",
       "  'be',\n",
       "  'several',\n",
       "  'graph',\n",
       "  'can',\n",
       "  'represent',\n",
       "  'one',\n",
       "  'situation',\n",
       "  'but',\n",
       "  'many',\n",
       "  'different',\n",
       "  'explanation',\n",
       "  'depend',\n",
       "  'on',\n",
       "  'different',\n",
       "  'graph',\n",
       "  '.'],\n",
       " ['for', 'example', 'the', 'structure', 'of', 'a', 'atom', '.'],\n",
       " ['three',\n",
       "  'chemist',\n",
       "  'thomson',\n",
       "  'rutherford',\n",
       "  'and',\n",
       "  'bohr',\n",
       "  'they',\n",
       "  'give',\n",
       "  'different',\n",
       "  'model',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'be',\n",
       "  'clearly',\n",
       "  'expert',\n",
       "  'in',\n",
       "  'the',\n",
       "  'same',\n",
       "  'discipline',\n",
       "  'and',\n",
       "  'they',\n",
       "  'often',\n",
       "  'use',\n",
       "  'model',\n",
       "  'to',\n",
       "  'show',\n",
       "  'the',\n",
       "  'structure',\n",
       "  'of',\n",
       "  'atom',\n",
       "  'but',\n",
       "  'they',\n",
       "  'be',\n",
       "  'not',\n",
       "  'have',\n",
       "  'the',\n",
       "  'same',\n",
       "  'opinion',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'disagree',\n",
       "  'with',\n",
       "  'the',\n",
       "  'previous',\n",
       "  'model',\n",
       "  'and',\n",
       "  'extract',\n",
       "  'new',\n",
       "  'knowledge',\n",
       "  'from',\n",
       "  'the',\n",
       "  'same',\n",
       "  'fact',\n",
       "  '.'],\n",
       " ['and',\n",
       "  'with',\n",
       "  'the',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  'they',\n",
       "  'have',\n",
       "  'get',\n",
       "  'they',\n",
       "  'will',\n",
       "  'try',\n",
       "  'to',\n",
       "  'find',\n",
       "  'a',\n",
       "  'better',\n",
       "  'knowledge',\n",
       "  'by',\n",
       "  'use',\n",
       "  'reasoning',\n",
       "  'to',\n",
       "  'evaluate',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  '.'],\n",
       " ['that',\n",
       "  'be',\n",
       "  'the',\n",
       "  'process',\n",
       "  'of',\n",
       "  'argument',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'between',\n",
       "  'expert',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'other',\n",
       "  'knowledge',\n",
       "  'question',\n",
       "  'that',\n",
       "  'can',\n",
       "  'be',\n",
       "  'separate',\n",
       "  'from',\n",
       "  'the',\n",
       "  'title',\n",
       "  'question',\n",
       "  'be',\n",
       "  'to',\n",
       "  'what',\n",
       "  'extend',\n",
       "  'can',\n",
       "  'a',\n",
       "  'argument',\n",
       "  'be',\n",
       "  'form',\n",
       "  'between',\n",
       "  'expert',\n",
       "  'in',\n",
       "  'same',\n",
       "  'discipline',\n",
       "  'when',\n",
       "  'they',\n",
       "  'have',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'knowledge',\n",
       "  'question',\n",
       "  'can',\n",
       "  'be',\n",
       "  'argue',\n",
       "  'in',\n",
       "  'the',\n",
       "  'area',\n",
       "  'of',\n",
       "  'mathematic',\n",
       "  '.'],\n",
       " ['mathematics',\n",
       "  'knowledge',\n",
       "  'be',\n",
       "  'consider',\n",
       "  'as',\n",
       "  'reliable',\n",
       "  'as',\n",
       "  'science',\n",
       "  '.'],\n",
       " ['for',\n",
       "  'a',\n",
       "  'mathematic',\n",
       "  'knowledge',\n",
       "  'the',\n",
       "  '\"',\n",
       "  'fact',\n",
       "  '\"',\n",
       "  'be',\n",
       "  'entirely',\n",
       "  'different',\n",
       "  'than',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'science',\n",
       "  '.'],\n",
       " ['fact',\n",
       "  'to',\n",
       "  'the',\n",
       "  'mathematician',\n",
       "  'be',\n",
       "  '\"',\n",
       "  'logic',\n",
       "  '\"',\n",
       "  'as',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  'of',\n",
       "  'mathematic',\n",
       "  'be',\n",
       "  'gain',\n",
       "  'mostly',\n",
       "  'by',\n",
       "  'the',\n",
       "  'logical',\n",
       "  'prove',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'mathematics',\n",
       "  'the',\n",
       "  'evidence',\n",
       "  'of',\n",
       "  'some',\n",
       "  'theory',\n",
       "  'be',\n",
       "  'not',\n",
       "  'just',\n",
       "  'what',\n",
       "  'we',\n",
       "  'observe',\n",
       "  'but',\n",
       "  'also',\n",
       "  'we',\n",
       "  'create',\n",
       "  '.'],\n",
       " ['expert',\n",
       "  'of',\n",
       "  'math',\n",
       "  'mathematician',\n",
       "  'seldom',\n",
       "  'have',\n",
       "  'disagreement',\n",
       "  'on',\n",
       "  'the',\n",
       "  'law',\n",
       "  'of',\n",
       "  'math',\n",
       "  'because',\n",
       "  'they',\n",
       "  'be',\n",
       "  'prove',\n",
       "  'and',\n",
       "  'they',\n",
       "  'be',\n",
       "  'logical',\n",
       "  '.'],\n",
       " ['so',\n",
       "  'the',\n",
       "  'mathematic',\n",
       "  'knowledge',\n",
       "  'be',\n",
       "  'gain',\n",
       "  'by',\n",
       "  'reasoning',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'intuition',\n",
       "  'in',\n",
       "  'term',\n",
       "  'of',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  '.'],\n",
       " ['logic',\n",
       "  'mean',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  'of',\n",
       "  'math',\n",
       "  'need',\n",
       "  'to',\n",
       "  'be',\n",
       "  'meaningful',\n",
       "  'in',\n",
       "  'the',\n",
       "  'physic',\n",
       "  'and',\n",
       "  'not',\n",
       "  'against',\n",
       "  'other',\n",
       "  'theory',\n",
       "  'or',\n",
       "  'law',\n",
       "  'in',\n",
       "  'mathematics',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'basic',\n",
       "  'evaluation',\n",
       "  'of',\n",
       "  'a',\n",
       "  'knowledge',\n",
       "  'in',\n",
       "  'mathematics',\n",
       "  'be',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'whether',\n",
       "  'it',\n",
       "  'be',\n",
       "  'meaningful',\n",
       "  'or',\n",
       "  'not',\n",
       "  '.'],\n",
       " ['so', 'the', 'logic', 'be', 'very', 'important', 'in', 'mathematics', '.'],\n",
       " ['also',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'in',\n",
       "  'mathematics',\n",
       "  'be',\n",
       "  'abstract',\n",
       "  'and',\n",
       "  'compare',\n",
       "  'to',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'be',\n",
       "  'less',\n",
       "  'complex',\n",
       "  '.'],\n",
       " ['thus',\n",
       "  'the',\n",
       "  'expert',\n",
       "  'of',\n",
       "  'mathematic',\n",
       "  'will',\n",
       "  'barely',\n",
       "  'have',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  'gain',\n",
       "  'form',\n",
       "  'same',\n",
       "  'fact',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'seldom',\n",
       "  'have',\n",
       "  'disagreement',\n",
       "  'or',\n",
       "  'a',\n",
       "  'different',\n",
       "  'opinion',\n",
       "  'on',\n",
       "  'a',\n",
       "  'law',\n",
       "  'or',\n",
       "  'a',\n",
       "  'theory',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'do',\n",
       "  'not',\n",
       "  'need',\n",
       "  'to',\n",
       "  'have',\n",
       "  'question',\n",
       "  'about',\n",
       "  'the',\n",
       "  'basic',\n",
       "  'knowledge',\n",
       "  'or',\n",
       "  'use',\n",
       "  'a',\n",
       "  'real',\n",
       "  'life',\n",
       "  'example',\n",
       "  'to',\n",
       "  'prove',\n",
       "  'a',\n",
       "  'law',\n",
       "  'be',\n",
       "  'incorrect',\n",
       "  '.'],\n",
       " ['for',\n",
       "  'example',\n",
       "  'my',\n",
       "  'teacher',\n",
       "  'ask',\n",
       "  'I',\n",
       "  'a',\n",
       "  'question',\n",
       "  'whether',\n",
       "  '\"',\n",
       "  '\"',\n",
       "  'be',\n",
       "  'always',\n",
       "  'true',\n",
       "  'and',\n",
       "  'i',\n",
       "  'say',\n",
       "  'no',\n",
       "  '.'],\n",
       " ['because',\n",
       "  'in',\n",
       "  'real',\n",
       "  'life',\n",
       "  'drop',\n",
       "  'of',\n",
       "  'water',\n",
       "  'plus',\n",
       "  'another',\n",
       "  'drop',\n",
       "  'of',\n",
       "  'water',\n",
       "  'be',\n",
       "  'still',\n",
       "  'drop',\n",
       "  '.'],\n",
       " ['however',\n",
       "  'this',\n",
       "  'can',\n",
       "  'not',\n",
       "  'prove',\n",
       "  'that',\n",
       "  'be',\n",
       "  'wrong',\n",
       "  'in',\n",
       "  'mathematics',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'reason',\n",
       "  'for',\n",
       "  'that',\n",
       "  'be',\n",
       "  'the',\n",
       "  'be',\n",
       "  'a',\n",
       "  'axiom',\n",
       "  'in',\n",
       "  'mathematic',\n",
       "  'which',\n",
       "  'be',\n",
       "  'the',\n",
       "  'law',\n",
       "  'that',\n",
       "  'we',\n",
       "  'create',\n",
       "  '.'],\n",
       " ['even',\n",
       "  'mathematician',\n",
       "  'know',\n",
       "  'be',\n",
       "  'not',\n",
       "  'correct',\n",
       "  'all',\n",
       "  'the',\n",
       "  'time',\n",
       "  'they',\n",
       "  'will',\n",
       "  'not',\n",
       "  'argue',\n",
       "  'as',\n",
       "  'it',\n",
       "  'be',\n",
       "  'not',\n",
       "  'a',\n",
       "  'fact',\n",
       "  'in',\n",
       "  'math',\n",
       "  '.'],\n",
       " ['there',\n",
       "  'be',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'axiom',\n",
       "  'in',\n",
       "  'mathematic',\n",
       "  'like',\n",
       "  'which',\n",
       "  'mathematician',\n",
       "  'all',\n",
       "  'agree',\n",
       "  'with',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'my',\n",
       "  'ib',\n",
       "  'class',\n",
       "  'of',\n",
       "  'mathematic',\n",
       "  'there',\n",
       "  'be',\n",
       "  'actual',\n",
       "  'example',\n",
       "  'that',\n",
       "  'mathematician',\n",
       "  'argue',\n",
       "  'with',\n",
       "  'each',\n",
       "  'other',\n",
       "  'in',\n",
       "  'history',\n",
       "  '.'],\n",
       " ['even',\n",
       "  'in',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'logical',\n",
       "  'area',\n",
       "  'of',\n",
       "  'knowledge',\n",
       "  'the',\n",
       "  'mathematician',\n",
       "  'will',\n",
       "  'have',\n",
       "  'argument',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'cantor',\n",
       "  'and',\n",
       "  'kronecker',\n",
       "  'have',\n",
       "  'a',\n",
       "  'long',\n",
       "  'argument',\n",
       "  'in',\n",
       "  'the',\n",
       "  'history',\n",
       "  '.'],\n",
       " ['they', 'study', 'in', 'a', 'same', 'field', 'in', 'mathematics', '.'],\n",
       " ['kronecker',\n",
       "  'disagree',\n",
       "  'with',\n",
       "  'cantor',\n",
       "  'as',\n",
       "  'cantor',\n",
       "  'have',\n",
       "  'create',\n",
       "  'a',\n",
       "  'axiom',\n",
       "  'which',\n",
       "  'against',\n",
       "  'he',\n",
       "  'study',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'reason',\n",
       "  'for',\n",
       "  'these',\n",
       "  'two',\n",
       "  'mathematician',\n",
       "  'argue',\n",
       "  'with',\n",
       "  'each',\n",
       "  'other',\n",
       "  'be',\n",
       "  'because',\n",
       "  'the',\n",
       "  'cantor',\n",
       "  'be',\n",
       "  'make',\n",
       "  'a',\n",
       "  'axiom',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'find',\n",
       "  'some',\n",
       "  'result',\n",
       "  'that',\n",
       "  'mathematician',\n",
       "  'agree',\n",
       "  'with',\n",
       "  'but',\n",
       "  'the',\n",
       "  'pattern',\n",
       "  'he',\n",
       "  'conclude',\n",
       "  'and',\n",
       "  'the',\n",
       "  'law',\n",
       "  'he',\n",
       "  'set',\n",
       "  'be',\n",
       "  'against',\n",
       "  'of',\n",
       "  'cantor',\n",
       "  'theory',\n",
       "  'lueck',\n",
       "  '.'],\n",
       " ['from',\n",
       "  'this',\n",
       "  'event',\n",
       "  'i',\n",
       "  'think',\n",
       "  'the',\n",
       "  'argument',\n",
       "  'be',\n",
       "  'cause',\n",
       "  'by',\n",
       "  'the',\n",
       "  'different',\n",
       "  'idea',\n",
       "  'between',\n",
       "  'expert',\n",
       "  'of',\n",
       "  'mathematic',\n",
       "  '.'],\n",
       " ['prove',\n",
       "  'a',\n",
       "  'theory',\n",
       "  'in',\n",
       "  'mathematic',\n",
       "  'can',\n",
       "  'be',\n",
       "  'very',\n",
       "  'complex',\n",
       "  'and',\n",
       "  'there',\n",
       "  'be',\n",
       "  'many',\n",
       "  'way',\n",
       "  'to',\n",
       "  'prove',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['for',\n",
       "  'example',\n",
       "  'cantor',\n",
       "  'support',\n",
       "  'he',\n",
       "  'result',\n",
       "  'without',\n",
       "  'give',\n",
       "  'a',\n",
       "  'explanation',\n",
       "  'of',\n",
       "  'what',\n",
       "  'be',\n",
       "  'infinity',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'cancel',\n",
       "  'the',\n",
       "  'infinity',\n",
       "  'term',\n",
       "  'on',\n",
       "  'both',\n",
       "  'side',\n",
       "  'of',\n",
       "  'he',\n",
       "  'equation',\n",
       "  'and',\n",
       "  'give',\n",
       "  'the',\n",
       "  'answer',\n",
       "  '.'],\n",
       " ['however',\n",
       "  'kronecker',\n",
       "  'think',\n",
       "  'that',\n",
       "  'he',\n",
       "  'do',\n",
       "  'not',\n",
       "  'give',\n",
       "  'the',\n",
       "  'meaning',\n",
       "  'of',\n",
       "  'infinite',\n",
       "  'so',\n",
       "  'it',\n",
       "  'can',\n",
       "  'not',\n",
       "  'be',\n",
       "  'logical',\n",
       "  'and',\n",
       "  'true',\n",
       "  '.'],\n",
       " ['as',\n",
       "  'the',\n",
       "  '\"',\n",
       "  'fact',\n",
       "  '\"',\n",
       "  'in',\n",
       "  'the',\n",
       "  'mathematic',\n",
       "  'be',\n",
       "  'logic',\n",
       "  'and',\n",
       "  'reason',\n",
       "  'if',\n",
       "  'a',\n",
       "  'mathematician',\n",
       "  'do',\n",
       "  'not',\n",
       "  'agree',\n",
       "  'with',\n",
       "  'the',\n",
       "  'logic',\n",
       "  'or',\n",
       "  'meaning',\n",
       "  'they',\n",
       "  'will',\n",
       "  'argue',\n",
       "  'with',\n",
       "  'other',\n",
       "  'expert',\n",
       "  '.'],\n",
       " ['also',\n",
       "  'the',\n",
       "  'different',\n",
       "  'between',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  'gain',\n",
       "  'from',\n",
       "  'same',\n",
       "  'fact',\n",
       "  'may',\n",
       "  'cause',\n",
       "  'by',\n",
       "  'intuition',\n",
       "  '.'],\n",
       " ['although',\n",
       "  'they',\n",
       "  'be',\n",
       "  'face',\n",
       "  'the',\n",
       "  'same',\n",
       "  'fact',\n",
       "  'in',\n",
       "  'mathematics',\n",
       "  'the',\n",
       "  'logic',\n",
       "  'they',\n",
       "  'use',\n",
       "  'be',\n",
       "  'different',\n",
       "  'which',\n",
       "  'cause',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  'they',\n",
       "  'could',\n",
       "  'gain',\n",
       "  '.'],\n",
       " ['as',\n",
       "  'they',\n",
       "  'have',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  'there',\n",
       "  'should',\n",
       "  'be',\n",
       "  'a',\n",
       "  'more',\n",
       "  'reliable',\n",
       "  'knowledge',\n",
       "  'so',\n",
       "  'they',\n",
       "  'begin',\n",
       "  'to',\n",
       "  'have',\n",
       "  'argument',\n",
       "  'for',\n",
       "  'a',\n",
       "  'improvement',\n",
       "  'in',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  'they',\n",
       "  'have',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'most',\n",
       "  'of',\n",
       "  'the',\n",
       "  'area',\n",
       "  'of',\n",
       "  'knowledge',\n",
       "  'the',\n",
       "  'expert',\n",
       "  'will',\n",
       "  'have',\n",
       "  'argument',\n",
       "  'with',\n",
       "  'each',\n",
       "  'other',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'expert',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'mainly',\n",
       "  'have',\n",
       "  'the',\n",
       "  'same',\n",
       "  'reason',\n",
       "  'for',\n",
       "  'argue',\n",
       "  'which',\n",
       "  'be',\n",
       "  'they',\n",
       "  'use',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  'the',\n",
       "  'reasoning',\n",
       "  'imagination',\n",
       "  'and',\n",
       "  'logic',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'can',\n",
       "  'use',\n",
       "  'a',\n",
       "  'different',\n",
       "  'combination',\n",
       "  'of',\n",
       "  'they',\n",
       "  'to',\n",
       "  'result',\n",
       "  'in',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  '.'],\n",
       " ['these',\n",
       "  'way',\n",
       "  'of',\n",
       "  'know',\n",
       "  'be',\n",
       "  'in',\n",
       "  'different',\n",
       "  'certainty',\n",
       "  'which',\n",
       "  'mean',\n",
       "  'reasoning',\n",
       "  'may',\n",
       "  'be',\n",
       "  'more',\n",
       "  'certain',\n",
       "  'than',\n",
       "  'logic',\n",
       "  'and',\n",
       "  'imagination',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'knowledge',\n",
       "  'they',\n",
       "  'gain',\n",
       "  'from',\n",
       "  'the',\n",
       "  'same',\n",
       "  'fact',\n",
       "  'will',\n",
       "  'have',\n",
       "  'different',\n",
       "  'certainty',\n",
       "  '.'],\n",
       " ['to',\n",
       "  'make',\n",
       "  'improvement',\n",
       "  'on',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  'the',\n",
       "  'scientist',\n",
       "  'begin',\n",
       "  'to',\n",
       "  'argue',\n",
       "  '.'],\n",
       " ['for',\n",
       "  'the',\n",
       "  'expert',\n",
       "  'of',\n",
       "  'mathematic',\n",
       "  'they',\n",
       "  'argue',\n",
       "  'when',\n",
       "  'they',\n",
       "  'can',\n",
       "  'not',\n",
       "  'agree',\n",
       "  'with',\n",
       "  'other',\n",
       "  'logic',\n",
       "  'in',\n",
       "  'other',\n",
       "  'mathematician',\n",
       "  'find',\n",
       "  '.'],\n",
       " ['similar',\n",
       "  'to',\n",
       "  'the',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'mathematician',\n",
       "  'also',\n",
       "  'need',\n",
       "  'improvement',\n",
       "  'on',\n",
       "  'they',\n",
       "  'knowledge',\n",
       "  'between',\n",
       "  'different',\n",
       "  'knowledge',\n",
       "  'cause',\n",
       "  'by',\n",
       "  'a',\n",
       "  'different',\n",
       "  'logic',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'argument',\n",
       "  'be',\n",
       "  'the',\n",
       "  'way',\n",
       "  'how',\n",
       "  'two',\n",
       "  'expert',\n",
       "  'evaluate',\n",
       "  'the',\n",
       "  'knowledge',\n",
       "  'they',\n",
       "  'have',\n",
       "  'and',\n",
       "  'develop',\n",
       "  'a',\n",
       "  'deeper',\n",
       "  'and',\n",
       "  'more',\n",
       "  'certain',\n",
       "  'knowledge',\n",
       "  'in',\n",
       "  'both',\n",
       "  'nature',\n",
       "  'science',\n",
       "  'and',\n",
       "  'mathematics',\n",
       "  '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_df.lemmatized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_p(list_of_list):\n",
    "    for i in list_of_list:\n",
    "        try:\n",
    "            i.remove(\".\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return list_of_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemma_df.lemmatized_text.apply(remove_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../essay_grader/data/essay_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>level</th>\n",
       "      <th>title_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The question is asking that, in the same disci...</td>\n",
       "      <td>2017</td>\n",
       "      <td>4, 5 - Est_Chen-fzn235-TOK_essay.docx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our brains seek coherence, structure, and orde...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Eva GuoTOK_final_final_draft.docx</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In American Heritage® Dictionary of the Englis...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Given access to the same facts, how is it poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The statement in the prompt argues that diffic...</td>\n",
       "      <td>2017</td>\n",
       "      <td>8, 1 - James Li TOK_Essay_4th_draft.docx</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>It is only knowledge produced with difficulty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Human are pattern-seeking animals because patt...</td>\n",
       "      <td>2017</td>\n",
       "      <td>7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Humans are pattern-seeking animals and we are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year  \\\n",
       "0  The question is asking that, in the same disci...  2017   \n",
       "1  Our brains seek coherence, structure, and orde...  2017   \n",
       "2  In American Heritage® Dictionary of the Englis...  2017   \n",
       "3  The statement in the prompt argues that diffic...  2017   \n",
       "4  Human are pattern-seeking animals because patt...  2017   \n",
       "\n",
       "                                                name  title  score  level  \\\n",
       "0              4, 5 - Est_Chen-fzn235-TOK_essay.docx      5      4      2   \n",
       "1           7, 6 - Eva GuoTOK_final_final_draft.docx      6      7      4   \n",
       "2  7, 5 - fzn260_Yessica_Ji_Yuanyi_G12-9_TOKEssay...      5      7      4   \n",
       "3           8, 1 - James Li TOK_Essay_4th_draft.docx      1      8      4   \n",
       "4  7, 6 - Fzn323_Amy_Wang_Qiaohui_G12_TOK_Essay_D...      6      7      4   \n",
       "\n",
       "                                          title_name  \n",
       "0  Given access to the same facts, how is it poss...  \n",
       "1  Humans are pattern-seeking animals and we are ...  \n",
       "2  Given access to the same facts, how is it poss...  \n",
       "3  It is only knowledge produced with difficulty ...  \n",
       "4  Humans are pattern-seeking animals and we are ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df = gen_text_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>vocab_richness</th>\n",
       "      <th>mean_word_syllable</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>word_commonness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.428571</td>\n",
       "      <td>3.306122</td>\n",
       "      <td>1.836735</td>\n",
       "      <td>0.311507</td>\n",
       "      <td>1.663672</td>\n",
       "      <td>1492.632653</td>\n",
       "      <td>71.673469</td>\n",
       "      <td>21.368343</td>\n",
       "      <td>690.510204</td>\n",
       "      <td>44.399184</td>\n",
       "      <td>9.957501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.681818</td>\n",
       "      <td>3.472727</td>\n",
       "      <td>3.581818</td>\n",
       "      <td>0.309585</td>\n",
       "      <td>1.691718</td>\n",
       "      <td>1485.168182</td>\n",
       "      <td>68.550000</td>\n",
       "      <td>22.235258</td>\n",
       "      <td>678.327273</td>\n",
       "      <td>41.146864</td>\n",
       "      <td>9.955442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.698113</td>\n",
       "      <td>3.578616</td>\n",
       "      <td>5.402516</td>\n",
       "      <td>0.309794</td>\n",
       "      <td>1.713382</td>\n",
       "      <td>1531.264151</td>\n",
       "      <td>67.905660</td>\n",
       "      <td>23.055620</td>\n",
       "      <td>691.220126</td>\n",
       "      <td>38.481509</td>\n",
       "      <td>9.951818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.796875</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>7.343750</td>\n",
       "      <td>0.314469</td>\n",
       "      <td>1.737963</td>\n",
       "      <td>1536.000000</td>\n",
       "      <td>67.515625</td>\n",
       "      <td>23.388957</td>\n",
       "      <td>691.421875</td>\n",
       "      <td>36.063594</td>\n",
       "      <td>9.951556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017.600000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.320593</td>\n",
       "      <td>1.715316</td>\n",
       "      <td>1554.800000</td>\n",
       "      <td>61.200000</td>\n",
       "      <td>26.369376</td>\n",
       "      <td>698.200000</td>\n",
       "      <td>34.952000</td>\n",
       "      <td>9.918938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              year     title     score  vocab_richness  mean_word_syllable  \\\n",
       "level                                                                        \n",
       "1      2018.428571  3.306122  1.836735        0.311507            1.663672   \n",
       "2      2018.681818  3.472727  3.581818        0.309585            1.691718   \n",
       "3      2018.698113  3.578616  5.402516        0.309794            1.713382   \n",
       "4      2018.796875  3.562500  7.343750        0.314469            1.737963   \n",
       "5      2017.600000  4.400000  9.000000        0.320593            1.715316   \n",
       "\n",
       "        word_count  sentence_count  avg_sentence_length  count_stopwords  \\\n",
       "level                                                                      \n",
       "1      1492.632653       71.673469            21.368343       690.510204   \n",
       "2      1485.168182       68.550000            22.235258       678.327273   \n",
       "3      1531.264151       67.905660            23.055620       691.220126   \n",
       "4      1536.000000       67.515625            23.388957       691.421875   \n",
       "5      1554.800000       61.200000            26.369376       698.200000   \n",
       "\n",
       "       flesch_reading_ease  word_commonness  \n",
       "level                                        \n",
       "1                44.399184         9.957501  \n",
       "2                41.146864         9.955442  \n",
       "3                38.481509         9.951818  \n",
       "4                36.063594         9.951556  \n",
       "5                34.952000         9.918938  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_df.groupby(\"level\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../essay_grader/pickle_data/text_feature_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

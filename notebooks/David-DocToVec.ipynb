{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string \n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    220\n",
       "3    159\n",
       "4     69\n",
       "1     49\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../essay_grader/data/essay_data.csv')\n",
    "#convert the 5 level 5 essays to a level 4\n",
    "df['level'] = df['level'].apply(lambda x: x if x < 4 else 4)\n",
    "df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0].count('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple pre-processing to use for Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    return text\n",
    "def remove_punctuation(review):\n",
    "    for punctuation in string.punctuation:\n",
    "        review = review.replace(punctuation, ' ')\n",
    "    return review\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text'] = df['text'].str.lower().apply(lambda x: remove_numbers(x)) #remove numbers\n",
    "df['text'] = df['text'].apply(remove_punctuation) #removes ENGLISH punctuations\n",
    "df['text'] = df['text'].str.replace('\\n', ' ') #remove new line characters\n",
    "df['text'] = df['text'].apply(lambda x: word_tokenize(x)) \n",
    "# df['text'] = df['text'].apply(lambda x:[w for w in x if w not in stop_words]) #can try to remove stop words if needed as well.\n",
    "df['text'] = df['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "\n",
    "#can try to use a stemmer here to see how it improve the model later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = ['level']), df['level'], random_state = 0, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create the vector and vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_create_vectors(text, dm, vector_size, negative, hs, min_count , sample , window):\n",
    "    \"\"\"\n",
    "    creates vectors from text files using the doc2vec model.\n",
    "    This is called by the doc2vec_vectorize function and returns a model as well as \n",
    "    documents which include the documents and the document tags.\n",
    "    Paramaters for the function are:\n",
    "    dm ({0,1}, optional) 1: PV-DM, 0: PV-DBOW \n",
    "    vector size, negative, hs, min_count, sample\n",
    "    \"\"\"\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(text)]\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model = Doc2Vec(window = window, dm = dm, vector_size = vector_size, negative = negative, hs = hs, min_count = min_count, sample = sample, works = cores)\n",
    "    model.build_vocab(documents)\n",
    "    model.train(documents, total_examples = len(documents), epochs = 30)\n",
    "    return model, documents\n",
    "\n",
    "def doc2vec_vectorize(df, col, dm =0, vector_size =300,  negative = 5, hs = 0, min_count = 2, sample = 0, window = 7):\n",
    "    \"\"\"function to vectorize the text using the doc2vec method.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.reset_index(inplace = True)\n",
    "    vecs = []\n",
    "    model, documents = d2v_create_vectors(df[col], dm, vector_size, negative, hs, min_count , sample , window)\n",
    "    for doc_id in range(len(documents)):\n",
    "        vecs.append(model.docvecs[doc_id])\n",
    "    new_col = 'd2v_' + col\n",
    "    df[new_col] = pd.Series(vecs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test to make sure the vecotrization worked\n",
    "- This checks to see which document each vector is most similar to.  The counter should show that almost all documents are most similar to themselves.\n",
    "\n",
    "- The print lines print the most simial document (it self usually) the second most, the median, and the least similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(documents)):\n",
    "    inferred_vector = model.infer_vector(documents[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 397})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(documents[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(documents[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### Tf-IDF with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize both the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question is asking that, in the same discipline, why experts often have different opinions on the same fact. For example, a scientist disagrees with other scientists’ discovers and results. Facts are the phenomena that can be commonly observed by us and can not be changed by us. They are the evidence and the basic knowledge that we can easily get. For example, the earth is a sphere. This is a fact that we already proved and observed from space. The evidence is sufficient to prove the fact by using reasoning as a way of knowing, and we now treat this fact as knowledge.\\n\\nThat\\'s how we gain knowledge from facts. However, facts are not the same in all disciplines. Based on the different ways of knowing used in different area of knowledge, facts are evaluated differently. Arguments are mainly caused by the different opinions or knowledge between two parties. Two experts may have disagreements on the understanding of the same phenomenon. Experts in the same discipline consist of different individuals, so they think differently which caused by many factors. They will have many different opinions on the same phenomena because the difference of using ways of knowing when experts gain knowledge from facts. When experts have different knowledge to each other, they will compare their knowledge and have arguments to find the “truth knowledge”. This is the general process of how an argument formed. Also, in different areas of knowledge, nature science, and mathematics, the arguments may be caused differently.\\n\\n\\n\\nThe main question can be separate into two knowledge questions. The first one is how the knowledge can be different from the same fact between experts in the same discipline. Experts in this area of knowledge are scientists. Science is considered as one of the most reliable knowledge because it based on lots of evidence. Facts, the evidence, are the center of the arguments between the experts. This means basically they will use reasoning as a way of knowing. However, in nature science, the arguments and disagreements always exist. In my IB biology class, although we studied the theory of evolution, but most of my classmates and even my teacher doesn’t agree with the certainty of the evolution theory which means what is accepted by some groups of scientists may not be accepted by others. In these case, it seems that the claim is correct. Based on nature science, to prove a theory to be wrong need evidence while to support still need evidence. That is because, in the area of nature science, we use reasoning as an important way of knowing. About evolution theory, we only have few direct evidence to prove its certainty but this evidence we have shows the evolution theory is the possible knowledge to be correct which means it is the most logical result we have. This knowledge is also now linked with imagination and intuition as a way of knowing. In nature science, when experts can not 100 percent relay on reasoning to gain knowledge, they will use intuition or imagination to gain knowledge from a fact. And different scientists may use different ways of knowing to gain knowledge. Some scientist may relay on reasoning more so they don’t agree with Evolution Theory. Some may relay on intuition and imagination more than reasoning. This cause the big difference between the knowledge they gain from the same fact. To prove a theory is wrong, a scientist can just show a counterclaim with enough evidence. This will ruin the logic and be giving reasons to complete a falsification. Thus, different knowledge is formed between different experts in nature science. In my IB chemistry class, the graph and models are always used to analyze different situation. We create models to explain some phenomenon we can not direct observe. There may be several graphs can represent one situation, but many different explanations depend on different graphs. For example, the structure of an atom. Three chemists, Thomson, Rutherford, and Bohr, they gave 3 different models. They are clearly experts in the same discipline and they often use models to show the structure of atoms, but they are not having the same opinion. They disagree with the previous model and extract new knowledge from the same fact. And with the different knowledge they have got, they will try to find a “better knowledge” by using reasoning to evaluate the knowledge. That is the process of arguments in nature science between experts.\\n\\n\\n\\nThe other knowledge question that can be separated from the title question is,” to what extend can an argument be formed between experts in same discipline when they have different knowledge.” This knowledge question can be argued in the area of mathematic. Mathematics knowledge is considered as reliable as science. For a mathematic knowledge, the \"fact\" is entirely different than in nature science. Facts to the mathematicians are \"logic\" as the knowledge of mathematic is gained mostly by the logical proves. In mathematics, the evidence of some theory are not just what we observed but also we create. Experts of math, mathematicians, seldom have disagreements on the laws of math because they are proved and they are logical.\\n\\nSo, the mathematic knowledge is gained by reasoning as well as intuition in terms of ways of knowing. Logic means the knowledge of math needs to be meaningful in the physics and not against other theories or laws in mathematics. The basic evaluation of a knowledge in mathematics is to determine whether it is meaningful or not. So the logic is very important in mathematics. Also, the fact in mathematics are abstract and comparing to the facts in nature science, the facts are less complex. Thus, the experts of mathematic will barely have different knowledge gain form same fact.\\n\\nThey seldom have disagreements or a different opinion on a law or a theory. They do not need to have questions about the basic knowledge or use a real life example to prove a law is incorrect. For example, my teacher asked me a question whether \"1+1=2\" is always true, and I said no. Because in real life, 1 drop of water plus another drop of water is still 1 drop. However, this can not prove that 1+1=2 is wrong in mathematics. The reason for that is the 1+1=2 is an axiom in mathematic which is the law that we create. Even mathematicians know “1+1=2” isn’t correct all the times, they will not argue as it is not a fact in math. There are lots of axiom in mathematic like “1+1=2” which mathematicians all agree with.\\n\\nIn my IB class of mathematic, there are actual examples that mathematicians argue with each other in history. Even in one of the most logical areas of knowledge, the mathematicians will have arguments. The Cantor and Kronecker had a long argument in the history. They studied in a same field in mathematics. Kronecker disagreed with Cantor as Cantor had created an axiom which against his study. The reason for these two mathematicians argue with each other is because the Cantor was making an axiom. He found some result that mathematicians agree with but the pattern he concluded and the law he set were against of Cantor’s theories (Lueck). From this event, I think the argument is caused by the different ideas between experts of mathematic. Proving a theory in mathematic can be very complex and there are many ways to prove it. For example, Cantor supports his result without giving an explanation of “what is infinity”. He canceled the infinity terms on both side of his equation and give the answer. However, Kronecker thought that he didn’t give the meaning of infinite, so it can’t be logical and true. As the \"fact\" in the mathematic is logics and reasons, if a mathematician doesn\\'t agree with the logic or meaning, they will argue with other experts. Also, the different between the knowledge gained from same fact may caused by intuition. Although they are facing the same fact in mathematics, the logic they use is different which causing different knowledge they could gain. As they have different knowledge, there should be a “more reliable” knowledge so they began to have arguments for an improvement in the knowledge they have.\\n\\nIn most of the areas of knowledge, the experts will have arguments with each other.\\n\\nThe experts in nature science mainly have the same reason for arguing which is they use 3 ways of knowing, the reasoning, imagination, and logic. They can use a different combination of them to result in different knowledge. These 3 ways of knowing are in different certainty which means reasoning may be more certain than logic and imagination. The knowledge they gain from the same fact will have different certainty. To make improvements on the knowledge, the scientist begins to argue.\\n\\nFor the experts of mathematic, they argue when they can not agree with other logic in other mathematicians’ finding. Similar to the nature science, mathematicians also need improvements on their knowledge between different knowledge caused by a different logic. The argument is the way how two experts evaluate the knowledge they have and develop a deeper and more certain knowledge in both nature science and mathematics.\\n\\n\\n\\n\\n\\nBibliography\\n\\n\\n\\nLueck, Kenzie. “Cantor versus Kronecker Debate.” Prezi, 2014.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#can choose doc2vec, Tf-Idf, or word2vec\n",
    "X_train_vecs = doc2vec_vectorize(X_train, 'text')\n",
    "X_test_vecs = doc2vec_vectorize(X_test, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validate one of the below models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.05619621, 0.05933285, 0.0507772 , 0.05084705, 0.04935598]),\n",
       " 'score_time': array([0.0096221 , 0.01016116, 0.00927591, 0.00996494, 0.00936723]),\n",
       " 'estimator': (SVC(kernel='sigmoid'),\n",
       "  SVC(kernel='sigmoid'),\n",
       "  SVC(kernel='sigmoid'),\n",
       "  SVC(kernel='sigmoid'),\n",
       "  SVC(kernel='sigmoid')),\n",
       " 'test_score': array([0.4       , 0.4625    , 0.40506329, 0.49367089, 0.35443038])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "svc_model = SVC(kernel = 'sigmoid')\n",
    "random_for_model = RandomForestClassifier()\n",
    "nb_log = MultinomialNB() #can try to Tf-Idf but not doc2vec\n",
    "\n",
    "\n",
    "cv = cross_validate(svc_model, X_train_vecs['d2v_text'].to_list(), y_train, cv = 5, n_jobs = -1, scoring = \"accuracy\", verbose = 1, return_estimator=True)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.50      0.79      0.61        48\n",
      "           3       0.25      0.22      0.24        27\n",
      "           4       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.44       100\n",
      "   macro avg       0.19      0.25      0.21       100\n",
      "weighted avg       0.31      0.44      0.36       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidchadsey/.pyenv/versions/3.7.7/envs/lewagon/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "estimator = cv['estimator'][4]\n",
    "y_pred = estimator.predict(X_test_vecs['d2v_text'].to_list())\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.14      0.08      0.10        13\n",
      "           2       0.48      0.52      0.50        48\n",
      "           3       0.21      0.22      0.21        27\n",
      "           4       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.35       100\n",
      "   macro avg       0.27      0.27      0.27       100\n",
      "weighted avg       0.34      0.35      0.34       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator = cv['estimator'][1]\n",
    "y_pred = estimator.predict(X_test_vecs)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
